# ğŸš€ Corrected DiffNet - 3-Branch Object Detection Architecture

A comprehensive object detection model implementing the complete 3-branch architecture with low-light enhancement, depth processing, and feature evolution for robust object detection.

## ğŸ—ï¸ Architecture Overview

### ğŸŒŸ **Luá»“ng 1 - Feature Mining (Encoder)**
```
Input Image â†’ Enhancement Module (ViT) â†’ f_E (features)
f_E â†’ Depth Module (Swin) â†’ f_D (features)  
f_E + f_D â†’ Feature Fusion â†’ f_ED (fused features)
```

### ğŸŒŸ **Luá»“ng 2 - Feature Evolution (Song song)**
```
Input Image + f_ED â†’ Transformation Module â†’ multi-scale features
features â†’ Recovery Module â†’ reconstructed image
```

### ğŸŒŸ **Luá»“ng 3 - Detection**
```
multi-scale features â†’ Detection Module â†’ Bounding boxes + Classes
```

## ğŸ“ Project Structure

```
Object_detection 2/
â”œâ”€â”€ configs/                 # YAML configuration files
â”‚   â”œâ”€â”€ model_tiny.yaml     # Tiny model (fast training)
â”‚   â”œâ”€â”€ model_small.yaml    # Small model (balanced)
â”‚   â”œâ”€â”€ model_base.yaml     # Base model (standard)
â”‚   â””â”€â”€ model_large.yaml    # Large model (best accuracy)
â”œâ”€â”€ models/                 # Model modules
â”‚   â”œâ”€â”€ detector.py         # Main detection model (CorrectedDiffNet)
â”‚   â””â”€â”€ modules/           # Individual model components
â”‚       â”œâ”€â”€ enhancement.py # Enhancement Module (ViT + ResNeXt)
â”‚       â”œâ”€â”€ depth.py       # Depth Module (Swin blocks)
â”‚       â”œâ”€â”€ evolution.py   # Feature Evolution + Fusion
â”‚       â””â”€â”€ common.py      # Shared utilities
â”œâ”€â”€ models/detection/       # Detection components
â”‚   â”œâ”€â”€ detection_module.py # Multi-scale Detection
â”‚   â”œâ”€â”€ head.py           # Detection Head
â”‚   â””â”€â”€ common.py         # Detection utilities
â”œâ”€â”€ utils/                 # Utility functions
â”‚   â”œâ”€â”€ config.py         # Configuration loader
â”‚   â”œâ”€â”€ datasets.py       # Dataset utilities
â”‚   â”œâ”€â”€ diffnet_loss.py   # Detection loss function
â”‚   â”œâ”€â”€ losses.py         # Additional losses
â”‚   â”œâ”€â”€ metrics.py        # COCO metrics
â”‚   â””â”€â”€ postprocess.py    # Post-processing utilities
â”œâ”€â”€ train.py              # Basic training script
â”œâ”€â”€ train_with_config.py  # Config-based training
â”œâ”€â”€ test_complete_training.py # Complete pipeline test + visualization
â””â”€â”€ requirements.txt      # Dependencies
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Install dependencies
pip install torch torchvision timm pyyaml matplotlib pillow opencv-python tqdm

# Or install from requirements.txt
pip install -r requirements.txt
```

### 2. Training

#### **Option A: Basic Training**
```bash
python train.py
```

#### **Option B: Config-Based Training (Recommended)**
```bash
# Train with different model sizes
python train_with_config.py --config tiny     # Fast training
python train_with_config.py --config small     # Balanced
python train_with_config.py --config base      # Standard
python train_with_config.py --config large     # Best accuracy

# Custom parameters
python train_with_config.py --config base --epochs 100 --batch_size 16 --lr 0.0001
```

### 3. Testing & Validation

#### **Complete Pipeline Test**
```bash
python test_complete_training.py
```

**This will:**
- Test model forward pass
- Run training with synthetic data
- Validate loss and metrics calculation
- Create bounding box visualizations
- Analyze training behavior

## ğŸ“Š Model Configurations

| Size | Parameters | Speed | Accuracy | Use Case |
|------|------------|-------|----------|---------|
| Tiny | ~5M | âš¡ Fast | Basic | Quick prototyping |
| Small | ~15M | ğŸš€ Fast | Good | Development |
| Base | ~30M | âš–ï¸ Medium | High | Production |
| Large | ~60M | ğŸŒ Slow | Best | Research |

## ğŸ”§ Configuration

### **Model Configuration (YAML)**
```yaml
# configs/model_base.yaml
model:
  name: "base"
  input_size: [224, 224]
  num_classes: 80

enhancement:
  embed_dim: 64
  num_heads: 8
  num_blocks: 3
  window_size: 7
  cardinality: 32

depth:
  embed_dim: 96
  depth: 3
  num_heads: 8

evolution:
  encoding_channels: [64, 128, 256]
  decoding_channels: [128, 64, 32]

detection:
  num_classes: 80
  fpn_channels: 256

training:
  batch_size: 16
  learning_rate: 0.0005
  epochs: 200
  weight_decay: 0.0005
```

### **Custom Configuration**
```python
from utils.config import load_config

# Load and modify config
config = load_config('base')
config.set('training.epochs', 100)
config.set('training.learning_rate', 0.001)
```

## ğŸ‹ï¸ Training Pipeline

### **Expected Training Behavior**
```
Epoch 1 Results:
  Train Loss: 0.4532
  Val mAP: 0.0123
  Val mAP50: 0.0456
  âœ… Loss decreasing: -0.1234
  âœ… mAP improving: 0.0123
```

### **Key Metrics**
- **Loss**: Should decrease from ~0.5 to ~0.1
- **mAP**: Should increase from 0 to >0.1
- **mAP50**: Should be higher than mAP (easier IoU threshold)
- **Confidence**: Should range from 0.1 to 0.9

### **Training Monitoring**
```python
# Training loop shows real-time metrics
pbar.set_postfix({
    "loss": f"{total_loss.item():.4f}",
    "det": f"{det_loss.item():.4f}",
    "rec": f"{rec_loss.item():.4f}"
})
```

## ğŸ¯ Model Components

### **Enhancement Module**
- **Input**: `[B, 3, H, W]` image
- **Output**: `[B, embed_dim, H/patch, W/patch]` features
- **Architecture**: ViT + ResNeXt with patch embedding
- **Key Features**: Low-light enhancement, attention mechanism

### **Depth Module**
- **Input**: Features from Enhancement Module
- **Output**: Enhanced depth features
- **Architecture**: Swin Transformer blocks
- **Key Features**: Hierarchical processing, self-attention

### **Feature Evolution**
- **Transformation**: Dual-input encoder (image + features)
- **Recovery**: Decoder with feature reconstruction
- **Key Features**: Feature fusion, multi-scale processing

### **Detection Head**
- **Input**: Multi-scale features from Transformation
- **Output**: Bounding boxes + classes + confidence
- **Architecture**: Feature Pyramid Network + multi-scale heads
- **Key Features**: Lightweight, accurate detection

## ğŸ“Š Inference & Results

### **Basic Inference**
```python
from models.detector import create_corrected_diffnet
import torch

# Load model
model = create_corrected_diffnet('base', num_classes=80)
model.load_state_dict(torch.load('model.pth')['model_state_dict'])
model.eval()

# Inference
image = torch.randn(1, 3, 224, 224)
with torch.no_grad():
    outputs = model(image)
    detections = model.detection_subnetwork.postprocess(
        outputs["raw_detect"], 
        conf_thresh=0.3, 
        iou_thresh=0.5
    )
```

### **Output Format**
```python
# detections[0] - First image in batch
tensor([[x1, y1, x2, y2, score, class_id],  # Detection 1
        [x1, y1, x2, y2, score, class_id],  # Detection 2
        ...])                           # More detections
```

### **Visualization**
The test script automatically creates visualizations showing:
- Ground truth boxes (green)
- Predicted boxes (colored by class)
- Confidence scores
- Class labels

## ğŸ”§ Dataset Setup

### **Dataset Format**
The model expects COCO-style datasets:

```json
{
  "images": [
    {"id": 1, "file_name": "image1.jpg", "height": 224, "width": 224},
    {"id": 2, "file_name": "image2.jpg", "height": 224, "width": 224}
  ],
  "annotations": [
    {
      "id": 1,
      "image_id": 1,
      "category_id": 1,
      "bbox": [x, y, width, height],
      "area": width * height
    }
  ]
}
```

### **Update Dataset Paths**
Edit `train.py` and `train_with_config.py`:

```python
# Update these paths to your dataset
train_dataset = COCODetectionDataset(
    img_dir="/path/to/your/dataset/train/img",
    ann_file="/path/to/your/dataset/train/annotations.json",
    transform=transform
)
```

## ğŸ› Troubleshooting

### **Common Issues & Solutions**

#### **Issue: Loss = 0**
- âœ… **Fixed**: Class indexing corrected
- âœ… **Fixed**: Anchor assignment improved
- âœ… **Fixed**: Head always available

#### **Issue: Metrics = 0**
- âœ… **Fixed**: Postprocess method correct
- âœ… **Fixed**: Detection format correct
- âœ… **Fixed**: Confidence threshold reasonable

#### **Issue: head = None**
- âœ… **Fixed**: Head always assigned to first detection head

#### **Issue: No Detections**
- ğŸ”§ **Solution**: Lower confidence threshold
- ğŸ”§ **Solution**: Check model outputs
- ğŸ”§ **Solution**: Verify dataset labels

### **Debug Commands**
```bash
# Check model structure
python -c "
from models.detector import create_corrected_diffnet
model = create_corrected_diffnet('tiny', num_classes=5)
print('Model created successfully')
"

# Test complete pipeline
python test_complete_training.py
```

## ğŸ“ˆ Performance

### **Expected Results (Synthetic Data)**
- **mAP**: 0.1 - 0.4
- **mAP50**: 0.2 - 0.6
- **mAP75**: 0.05 - 0.3
- **Recall**: 0.3 - 0.8

### **Training Speed**
| Model | Batch Size | GPU Memory | Training Time |
|-------|------------|------------|-------------|
| Tiny | 32 | 2GB | ~2 hours |
| Small | 16 | 4GB | ~4 hours |
| Base | 8 | 8GB | ~8 hours |
| Large | 4 | 16GB | ~16 hours |

## ğŸ¯ Advanced Usage

### **Custom Model Creation**
```python
from models.detector import CorrectedDiffNet

model = CorrectedDiffNet(
    num_classes=10,
    img_size=(256, 256),
    enhancement_cfg={
        'embed_dim': 96,
        'num_heads': 12,
        'num_blocks': 4
    },
    depth_cfg={
        'depth': 4,
        'embed_dim': 128
    }
)
```

### **Feature Extraction**
```python
# Extract intermediate features
outputs = model(image)

# Access different components
enhancement_features = outputs['enhancement_features']
depth_features = outputs['depth_features']
multi_scale_features = outputs['multi_scale_features']
reconstructed = outputs['reconstructed']
```

## ğŸ“š Dependencies

- **PyTorch** >= 1.9.0
- **torchvision** >= 0.10.0
- **timm** >= 0.6.0
- **PyYAML** >= 6.0
- **matplotlib** >= 3.0.0
- **Pillow** >= 8.0.0
- **OpenCV** >= 4.5.0
- **tqdm** >= 4.0.0

## ğŸ† Features

- âœ… **Correct 3-Branch Architecture** - Exactly as specified
- âœ… **Feature Mining** - ViT + Swin + Feature Fusion
- âœ… **Feature Evolution** - Dual-input Transformation + Recovery
- âœ… **Multi-scale Detection** - FPN + Lightweight heads
- âœ… **YAML Configuration** - Flexible model management
- âœ… **Complete Pipeline** - Training + Validation + Inference
- âœ… **Visualization** - Bounding box drawing
- âœ… **Multiple Model Sizes** - Tiny to Large
- âœ… **Robust Loss Functions** - Improved anchor assignment
- âœ… **COCO Metrics** - Standard evaluation

## ğŸš€ Getting Started

### **1. Quick Test**
```bash
python test_complete_training.py
```

### **2. Prepare Dataset**
- Organize dataset in COCO format
- Update paths in training scripts
- Verify data loading

### **3. Start Training**
```bash
# Start with small model for testing
python train_with_config.py --config small --epochs 10

# Full training
python train_with_config.py --config base --epochs 100
```

### **4. Evaluate Results**
- Monitor training loss and metrics
- Check visualizations in test_results/
- Analyze final model performance

---

## ğŸ‰ Ready to Train!

Your corrected DiffNet model is ready for training with:
- âœ… **Proper 3-branch architecture**
- âœ… **Fixed loss and metrics calculation**
- âœ… **Complete training pipeline**
- âœ… **Visualization and testing tools**
- âœ… **Multiple model sizes**
- âœ… **Robust loss functions**
- âœ… **COCO metrics**

**Start training now and see your model learn to detect objects!** ğŸš€
